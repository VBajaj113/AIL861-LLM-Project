{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699e633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Courses\\Sem 9\\AIL\\Project\\.conda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    " # server.py\n",
    "import asyncio\n",
    "import csv\n",
    "import gc\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import llama_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8ffd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"./models/\"\n",
    "\n",
    "# base LM for all experts (must be compatible with the adapters)\n",
    "BASE_LM_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "ORCHESTRATOR_MODEL_PATH = MODELS_DIR\n",
    "\n",
    "EXPERT_ADAPTERS = {\n",
    "    \"anxiety\": os.path.join(MODELS_DIR, \"anxiety\"),\n",
    "    \"bipolar\": os.path.join(MODELS_DIR, \"bipolar\"),\n",
    "    \"depression\": os.path.join(MODELS_DIR, \"depression\"),\n",
    "    \"ocd\": os.path.join(MODELS_DIR, \"ocd\"),\n",
    "    \"schizophrenia\": os.path.join(MODELS_DIR, \"schizophrenia\"),\n",
    "}\n",
    "\n",
    "LABEL2ID = {\n",
    "    \"anxiety\": 0,\n",
    "    \"bipolar\": 1,\n",
    "    \"depression\": 2,\n",
    "    \"ocd\": 3,\n",
    "    \"schizophrenia\": 4,\n",
    "}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # CPU-only as requested\n",
    "\n",
    "# --- toggles for experiments ---\n",
    "\n",
    "# 1) MEMORY LIMITING: max number of experts loaded at once\n",
    "ENABLE_MEMORY_LIMIT = True\n",
    "MAX_LOADED_EXPERTS = 2  # change to 1..5 ; if 5 == no effective limit\n",
    "\n",
    "# 2) MICRO-BATCHING per expert (simple, small batches)\n",
    "ENABLE_BATCHING = False\n",
    "MAX_BATCH_SIZE = 4\n",
    "MAX_BATCH_WAIT_S = 0.005   # how long a worker waits to fill batch\n",
    "\n",
    "# 3) Logging\n",
    "LOG_FILE_SERVER = \"./logs/server_logs.csv\"\n",
    "\n",
    "BASE_MODEL_PATH = \"./models/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e01c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcfc9d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorRouter:\n",
    "    def __init__(self, model_path: str):\n",
    "        labelmap_path = model_path + \"label_mapping.json\"\n",
    "        model_path = model_path + \"orchestrator.pkl\"\n",
    "\n",
    "        pipe = joblib.load(model_path)\n",
    "        with open(labelmap_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            mapping = json.load(f)\n",
    "        label2id = mapping[\"LABEL2ID\"]\n",
    "        id2label = {int(k): v for k, v in mapping[\"ID2LABEL\"].items()}\n",
    "\n",
    "        self.model = pipe\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "\n",
    "    def route(self, text: str):\n",
    "        probs = self.model.predict([text])\n",
    "        print(f\"[ORCH] Routing probs: {probs}\")\n",
    "        label_id = probs[0]\n",
    "        label_str = self.id2label[label_id]\n",
    "        return label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f182e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BatchJob:\n",
    "    request_id: str\n",
    "    func_id: int\n",
    "    prompt: str\n",
    "    enqueue_time: float\n",
    "    future: asyncio.Future\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExpertModel:\n",
    "    name: str\n",
    "    adapter_path: str\n",
    "    tokenizer: AutoTokenizer\n",
    "    base_lm_name: str = BASE_LM_NAME\n",
    "\n",
    "    model: Optional[AutoModelForCausalLM] = field(default=None, init=False)\n",
    "    lock: threading.Lock = field(default_factory=threading.Lock, init=False)\n",
    "\n",
    "    # for batching\n",
    "    request_queue: Optional[asyncio.Queue] = field(default=None, init=False)\n",
    "    worker_task: Optional[asyncio.Task] = field(default=None, init=False)\n",
    "\n",
    "    def ensure_loaded(self) -> float:\n",
    "        \"\"\"\n",
    "        Lazily load PEFT model. Returns load time in ms (0 if already loaded).\n",
    "        Also updates global LRU for memory limiting.\n",
    "        \"\"\"\n",
    "        global loaded_lru, experts\n",
    "\n",
    "        if self.model is not None:\n",
    "            # mark LRU touch even when already loaded\n",
    "            mark_expert_used(self.name)\n",
    "            return 0.0\n",
    "\n",
    "        print(f\"[MEM] Loading expert '{self.name}' from {self.adapter_path}...\")\n",
    "        start = time.perf_counter()\n",
    "        with self.lock:\n",
    "            if self.model is None:\n",
    "                print(f\"[MEM] Actually loading expert '{self.name}'...\")\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    BASE_MODEL_PATH,\n",
    "                    torch_dtype=torch.float32,\n",
    "                    device_map={\"\": DEVICE},\n",
    "                    token=HF_TOKEN,\n",
    "                )\n",
    "                print(f\"[MEM] Base model loaded for expert '{self.name}'\")\n",
    "                self.model = PeftModel.from_pretrained(\n",
    "                    base_model,\n",
    "                    self.adapter_path,\n",
    "                ).to(DEVICE)\n",
    "                \n",
    "                self.model.eval()\n",
    "\n",
    "                mark_expert_used(self.name)\n",
    "        end = time.perf_counter()\n",
    "        return (end - start) * 1000.0\n",
    "\n",
    "    def unload(self):\n",
    "        \"\"\"Free the model to simulate memory pressure.\"\"\"\n",
    "        if self.model is not None:\n",
    "            print(f\"[MEM] Unloading expert '{self.name}'\")\n",
    "            self.model = None\n",
    "            gc.collect()\n",
    "\n",
    "    # -------- inference helpers --------\n",
    "\n",
    "    def generate_batch(self, prompts: List[str], max_new_tokens: int = 128) -> List[str]:\n",
    "        assert self.model is not None, \"Model not loaded\"\n",
    "\n",
    "        # 1. Build chat-style inputs\n",
    "        conversations = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
    "\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            conversations,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            add_generation_prompt=True,  # leaves it at the assistant turn\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # 2. Explicit attention mask (since pad == eos)\n",
    "        attn_mask = (input_ids != self.tokenizer.pad_token_id).long().to(DEVICE)\n",
    "\n",
    "        input_len = input_ids.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attn_mask,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                top_p=0.9,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "        # 3. Drop the prompt part → keep only new assistant tokens\n",
    "        gen_only = output_ids[:, input_len:]\n",
    "\n",
    "        texts = self.tokenizer.batch_decode(gen_only, skip_special_tokens=True)\n",
    "        # optional clean-up whitespace\n",
    "        texts = [t.strip() for t in texts]\n",
    "        return texts\n",
    "\n",
    "\n",
    "\n",
    "    def generate_single(self, prompt: str, max_new_tokens: int = 128) -> str:\n",
    "        return self.generate_batch([prompt], max_new_tokens=max_new_tokens)[0]\n",
    "\n",
    "# --- LRU memory book-keeping ---\n",
    "\n",
    "loaded_lru: deque = deque()  # left = LRU, right = MRU\n",
    "experts: Dict[str, ExpertModel] = {}  # filled on startup\n",
    "\n",
    "\n",
    "def mark_expert_used(name: str):\n",
    "    \"\"\"Update LRU order and evict experts if over limit.\"\"\"\n",
    "    if not ENABLE_MEMORY_LIMIT:\n",
    "        return\n",
    "\n",
    "    # move name to right (MRU)\n",
    "    if name in loaded_lru:\n",
    "        loaded_lru.remove(name)\n",
    "    loaded_lru.append(name)\n",
    "\n",
    "    # evict until limit satisfied\n",
    "    while len(loaded_lru) > MAX_LOADED_EXPERTS:\n",
    "        evict_name = loaded_lru.popleft()\n",
    "        if evict_name == name:\n",
    "            # shouldn't happen, but guard\n",
    "            continue\n",
    "        experts[evict_name].unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14542bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Courses\\Sem 9\\AIL\\Project\\.conda\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1025: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "orchestrator = OrchestratorRouter(ORCHESTRATOR_MODEL_PATH)\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_LM_NAME, use_auth_token=HF_TOKEN)\n",
    "base_tokenizer.pad_token = base_tokenizer.special_tokens_map[\"eos_token\"]\n",
    "base_tokenizer.padding_side = \"left\"\n",
    "\n",
    "experts = {\n",
    "    label: ExpertModel(\n",
    "        name=label,\n",
    "        adapter_path=adapter_path,\n",
    "        tokenizer=base_tokenizer,\n",
    "    )\n",
    "    for label, adapter_path in EXPERT_ADAPTERS.items()\n",
    "}\n",
    "\n",
    "# init queues + workers for batching (if enabled)\n",
    "if ENABLE_BATCHING:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    for exp in experts.values():\n",
    "        exp.request_queue = asyncio.Queue()\n",
    "        exp.worker_task = loop.create_task(batch_worker(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f622d2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORCH] Routing probs: [3]\n",
      "Routed to expert 'ocd'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"how to have sex with a partner?\"\n",
    "label = orchestrator.route(prompt)\n",
    "print(f\"Routed to expert '{label}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff1b4628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MEM] Loading expert 'ocd' from ./models/ocd...\n",
      "[MEM] Actually loading expert 'ocd'...\n",
      "[MEM] Base model loaded for expert 'ocd'\n"
     ]
    }
   ],
   "source": [
    "chosen_expert = experts[label]\n",
    "queue_time_ms = 0.0\n",
    "batch_size = 1\n",
    "load_time_ms = await asyncio.to_thread(chosen_expert.ensure_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb254a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await asyncio.to_thread(chosen_expert.generate_single, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fd16f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When a partner is experiencing compulsive or excessive urges to engage in behaviors (e.g., engaging in auto-sexual rituals, feeling pleasure from sex), the therapist should guide the individual to have sex while they are in a state of normal arousal and while the urges are not overwhelming. This means the couple would work together to manage the urges and reduce their intensity before engaging in intercourse. If the urges are too overwhelming and the urges are so intense that the therapist fears for the couple's safety, a couples therapy approach might be necessary. The therapist should emphasize the importance of mutual support and communication during this challenging time. The couple might also benefit from\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c6354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.py\n",
    "import asyncio\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import httpx\n",
    "\n",
    "TRACE_FILE = \"./client_traces/trace1.csv\"\n",
    "PROMPTS_FILE = \"./client_traces/prompts.json\"\n",
    "CLIENT_LOG_FILE = \"./logs/client_logs.csv\"\n",
    "SERVER_URL = \"http://localhost:8000/infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22baa142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Logging\n",
    "# ============================================================\n",
    "\n",
    "class ClientCsvLogger(threading.Thread):\n",
    "    def __init__(self, log_queue: queue.Queue, filename: str, fieldnames):\n",
    "        super().__init__(daemon=True)\n",
    "        self.log_queue = log_queue\n",
    "        self.filename = filename\n",
    "        self.fieldnames = fieldnames\n",
    "\n",
    "    def run(self):\n",
    "        os.makedirs(os.path.dirname(self.filename) or \".\", exist_ok=True)\n",
    "        file_exists = os.path.exists(self.filename)\n",
    "        with open(self.filename, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "\n",
    "            while True:\n",
    "                record = self.log_queue.get()\n",
    "                if record is None:\n",
    "                    break\n",
    "                writer.writerow(record)\n",
    "                f.flush()\n",
    "\n",
    "\n",
    "client_log_queue = queue.Queue()\n",
    "client_logger = ClientCsvLogger(\n",
    "    client_log_queue,\n",
    "    CLIENT_LOG_FILE,\n",
    "    fieldnames=[\n",
    "        \"client_send_ts\",\n",
    "        \"request_id\",\n",
    "        \"func_id\",\n",
    "        \"trace_start_time\",\n",
    "        \"prompt\",\n",
    "        \"response_label\",\n",
    "        \"response_expert\",\n",
    "        \"e2e_time_ms\",\n",
    "        \"server_total_time_ms\",\n",
    "        \"batch_size\",\n",
    "    ],\n",
    ")\n",
    "client_logger.start()\n",
    "\n",
    "\n",
    "def log_client(record):\n",
    "    client_log_queue.put(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Trace + prompts\n",
    "# ============================================================\n",
    "\n",
    "def load_prompts(path: str) -> Dict[int, List[str]]:\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "    return {int(k): v for k, v in raw.items()}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Worker per trace entry\n",
    "# ============================================================\n",
    "\n",
    "async def send_request(\n",
    "    client: httpx.AsyncClient,\n",
    "    t0: float,\n",
    "    first_start: float,\n",
    "    prompts_by_func: Dict[int, List[str]],\n",
    "):\n",
    "    # respect relative timing from trace\n",
    "    relative_start = time.perf_counter() - first_start\n",
    "    now = time.perf_counter()\n",
    "    delay = relative_start - (now - t0)\n",
    "    if delay > 0:\n",
    "        await asyncio.sleep(delay)\n",
    "\n",
    "    prompts = prompts_by_func.get(random.randint(0,4))\n",
    "    prompt = random.choice(prompts)\n",
    "\n",
    "    send_ts = time.time()\n",
    "    t_start = time.perf_counter()\n",
    "\n",
    "    payload = {\n",
    "        \"request_id\": 0,\n",
    "        \"func_id\": 0,\n",
    "        \"start_time\": 0,\n",
    "        \"prompt\": prompt,\n",
    "    }\n",
    "\n",
    "    resp = await client.post(SERVER_URL, json=payload, timeout=None)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    t_end = time.perf_counter()\n",
    "    e2e_ms = (t_end - t_start) * 1000.0\n",
    "\n",
    "    timings = data.get(\"timings_ms\", {})\n",
    "    server_total = timings.get(\"total_time_ms\", None)\n",
    "    batch_size = data.get(\"batch_size\", 1)\n",
    "\n",
    "    log_client(\n",
    "        {\n",
    "            \"client_send_ts\": send_ts,\n",
    "            \"request_id\": 0,\n",
    "            \"func_id\": 0,\n",
    "            \"trace_start_time\": 0,\n",
    "            \"prompt\": prompt,\n",
    "            \"response_label\": data.get(\"chosen_label\"),\n",
    "            \"response_expert\": data.get(\"chosen_expert\"),\n",
    "            \"e2e_time_ms\": e2e_ms,\n",
    "            \"server_total_time_ms\": server_total,\n",
    "            \"batch_size\": batch_size,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[{0}] func={0} \"\n",
    "        f\"label={data.get('chosen_label')} expert={data.get('chosen_expert')} \"\n",
    "        f\"batch={batch_size} e2e={e2e_ms:.1f}ms server={server_total:.1f}ms\"\n",
    "    )\n",
    "\n",
    "prompts_by_func = load_prompts(PROMPTS_FILE)\n",
    "\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "async with httpx.AsyncClient() as client:\n",
    "    tasks = [\n",
    "        asyncio.create_task(\n",
    "            send_request(client, t0, 0, prompts_by_func)\n",
    "        )\n",
    "    ]\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e14566c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# server_gguf.py\n",
    "import asyncio\n",
    "import csv\n",
    "import gc\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from llama_cpp import Llama\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Orchestrator dir: contains orchestrator.pkl + label_mapping.json\n",
    "ORCHESTRATOR_MODEL_PATH = \"./models/\"\n",
    "\n",
    "# Map each label to its GGUF model path\n",
    "GGUF_MODELS = {\n",
    "    \"anxiety\": \"./models/gguf/anxiety-q8_0.gguf\",\n",
    "    \"bipolar\": \"./models/gguf/bipolar-q8_0.gguf\",\n",
    "    \"depression\": \"./models/gguf/depression-q8_0.gguf\",\n",
    "    \"ocd\": \"./models/gguf/ocd-q8_0.gguf\",\n",
    "    \"schizophrenia\": \"./models/gguf/schizophrenia-q8_0.gguf\",\n",
    "}\n",
    "\n",
    "# Hard limit on how many GGUF models can be resident at once\n",
    "ENABLE_MEMORY_LIMIT = True\n",
    "MAX_LOADED_MODELS = 2  # you said more than 2 explodes RAM\n",
    "\n",
    "# Batching per expert\n",
    "ENABLE_BATCHING = False\n",
    "MAX_BATCH_SIZE = 4\n",
    "MAX_BATCH_WAIT_S = 0.05  # how long worker waits to accumulate batch\n",
    "\n",
    "# llama.cpp params (tune for your CPU)\n",
    "N_CTX = 4096\n",
    "N_THREADS = 8\n",
    "N_PARALLEL = 1  # llama.cpp internal parallel sequences; keep 1 for safety here\n",
    "\n",
    "LOG_FILE_SERVER = \"./logs/server_gguf_logs.csv\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Logging infra\n",
    "# ============================================================\n",
    "\n",
    "class CsvLoggerThread(threading.Thread):\n",
    "    def __init__(self, log_queue: queue.Queue, filename: str, fieldnames: List[str]):\n",
    "        super().__init__(daemon=True)\n",
    "        self.log_queue = log_queue\n",
    "        self.filename = filename\n",
    "        self.fieldnames = fieldnames\n",
    "\n",
    "    def run(self):\n",
    "        os.makedirs(os.path.dirname(self.filename) or \".\", exist_ok=True)\n",
    "        file_exists = os.path.exists(self.filename)\n",
    "        with open(self.filename, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "                f.flush()\n",
    "            while True:\n",
    "                record = self.log_queue.get()\n",
    "                if record is None:\n",
    "                    break\n",
    "                writer.writerow(record)\n",
    "                f.flush()\n",
    "\n",
    "\n",
    "server_log_queue = queue.Queue()\n",
    "server_logger = CsvLoggerThread(\n",
    "    server_log_queue,\n",
    "    LOG_FILE_SERVER,\n",
    "    fieldnames=[\n",
    "        \"server_receive_ts\",\n",
    "        \"request_id\",\n",
    "        \"func_id\",\n",
    "        \"prompt_len\",\n",
    "        \"chosen_label\",\n",
    "        \"chosen_expert\",\n",
    "        \"orchestrator_time_ms\",\n",
    "        \"preprocess_time_ms\",\n",
    "        \"queue_time_ms\",\n",
    "        \"load_time_ms\",\n",
    "        \"inference_time_ms\",\n",
    "        \"total_time_ms\",\n",
    "        \"batch_size\",\n",
    "    ],\n",
    ")\n",
    "server_logger.start()\n",
    "\n",
    "\n",
    "def log_server(record: Dict):\n",
    "    server_log_queue.put(record)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Orchestrator\n",
    "# ============================================================\n",
    "\n",
    "class OrchestratorRouter:\n",
    "    def __init__(self, model_dir: str):\n",
    "        labelmap_path = os.path.join(model_dir, \"label_mapping.json\")\n",
    "        model_path = os.path.join(model_dir, \"orchestrator.pkl\")\n",
    "\n",
    "        pipe = joblib.load(model_path)\n",
    "        with open(labelmap_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            mapping = json.load(f)\n",
    "\n",
    "        self.model = pipe\n",
    "        self.label2id = mapping[\"LABEL2ID\"]\n",
    "        self.id2label = {int(k): v for k, v in mapping[\"ID2LABEL\"].items()}\n",
    "\n",
    "    def route(self, text: str) -> str:\n",
    "        pred = self.model.predict([text])\n",
    "        label_id = pred[0]\n",
    "        return self.id2label[label_id]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Global model state for memory limiting\n",
    "# ============================================================\n",
    "\n",
    "experts: Dict[str, \"GgufExpert\"] = {}  # filled on startup\n",
    "\n",
    "# Protects: num_loaded, expert.active_count, expert.llm\n",
    "global_cond = threading.Condition()\n",
    "num_loaded = 0  # how many GGUF models are currently loaded (llm != None)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Expert models (GGUF via llama.cpp)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class BatchJob:\n",
    "    request_id: str\n",
    "    func_id: int\n",
    "    prompt: str\n",
    "    enqueue_time: float\n",
    "    future: asyncio.Future\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GgufExpert:\n",
    "    name: str\n",
    "    model_path: str\n",
    "\n",
    "    llm: Optional[Llama] = field(default=None, init=False)\n",
    "    active_count: int = field(default=0, init=False)\n",
    "    # per-expert batching queue & worker\n",
    "    request_queue: Optional[asyncio.Queue] = field(default=None, init=False)\n",
    "    worker_task: Optional[asyncio.Task] = field(default=None, init=False)\n",
    "\n",
    "    # ---------- load accounting ----------\n",
    "\n",
    "    def inc_active(self):\n",
    "        global global_cond\n",
    "        with global_cond:\n",
    "            self.active_count += 1\n",
    "\n",
    "    def dec_active(self):\n",
    "        global global_cond\n",
    "        with global_cond:\n",
    "            self.active_count = max(0, self.active_count - 1)\n",
    "            global_cond.notify_all()\n",
    "\n",
    "    # ---------- memory-limited loading ----------\n",
    "\n",
    "    def ensure_loaded(self) -> float:\n",
    "        \"\"\"\n",
    "        Hard memory limit:\n",
    "        - At most MAX_LOADED_MODELS experts with llm != None.\n",
    "        - If limit is hit:\n",
    "            * Try to evict some other idle expert (active_count == 0).\n",
    "            * If none idle, WAIT until someone finishes and retry.\n",
    "        \"\"\"\n",
    "        global num_loaded, global_cond, experts, ENABLE_MEMORY_LIMIT, MAX_LOADED_MODELS\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        with global_cond:\n",
    "            # Fast path: already loaded\n",
    "            if self.llm is not None:\n",
    "                return 0.0\n",
    "\n",
    "            while True:\n",
    "                # maybe another thread loaded it while we waited\n",
    "                if self.llm is not None:\n",
    "                    return 0.0\n",
    "\n",
    "                if not ENABLE_MEMORY_LIMIT:\n",
    "                    num_loaded += 1\n",
    "                    break\n",
    "\n",
    "                if num_loaded < MAX_LOADED_MODELS:\n",
    "                    num_loaded += 1\n",
    "                    break\n",
    "\n",
    "                # Limit reached: try to evict some other idle expert\n",
    "                idle_exp: Optional[GgufExpert] = None\n",
    "                for other in experts.values():\n",
    "                    if other is self:\n",
    "                        continue\n",
    "                    if other.llm is not None and other.active_count == 0:\n",
    "                        idle_exp = other\n",
    "                        break\n",
    "\n",
    "                if idle_exp is not None:\n",
    "                    print(f\"[MEM] Evicting idle expert '{idle_exp.name}'\")\n",
    "                    idle_exp.llm = None\n",
    "                    num_loaded = max(0, num_loaded - 1)\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "\n",
    "                # All loaded experts busy: wait\n",
    "                global_cond.wait()\n",
    "\n",
    "        # We reserved a slot (num_loaded++). Load outside lock.\n",
    "        try:\n",
    "            print(f\"[MEM] Loading GGUF for expert '{self.name}' from {self.model_path}\")\n",
    "            llm = Llama(\n",
    "                model_path=self.model_path,\n",
    "                n_ctx=N_CTX,\n",
    "                n_threads=N_THREADS,\n",
    "                n_batch=MAX_BATCH_SIZE * 32,  # heuristic; tune if needed\n",
    "                n_gpu_layers=0,  # CPU only\n",
    "                logits_all=False,\n",
    "                vocab_only=False,\n",
    "                seed=0,\n",
    "            )\n",
    "        except Exception:\n",
    "            # rollback on failure\n",
    "            with global_cond:\n",
    "                num_loaded = max(0, num_loaded - 1)\n",
    "                global_cond.notify_all()\n",
    "            raise\n",
    "\n",
    "        # Commit model\n",
    "        with global_cond:\n",
    "            self.llm = llm\n",
    "            global_cond.notify_all()\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        return (end - start) * 1000.0\n",
    "\n",
    "    # ---------- unload (not called directly except via eviction) ----------\n",
    "\n",
    "    def unload(self):\n",
    "        global num_loaded, global_cond\n",
    "        with global_cond:\n",
    "            if self.llm is not None:\n",
    "                print(f\"[MEM] Unloading expert '{self.name}'\")\n",
    "                self.llm = None\n",
    "                num_loaded = max(0, num_loaded - 1)\n",
    "                gc.collect()\n",
    "                global_cond.notify_all()\n",
    "\n",
    "    # ---------- generate helpers (via llama.cpp) ----------\n",
    "\n",
    "    def _generate_one(self, prompt: str, max_new_tokens: int = 128) -> str:\n",
    "        assert self.llm is not None, \"llm not loaded\"\n",
    "        # Chat-style call so you get instruct behaviour\n",
    "        result = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_new_tokens,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "    def generate_single(self, prompt: str, max_new_tokens: int = 128) -> str:\n",
    "        # run blocking in current thread\n",
    "        return self._generate_one(prompt, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    def generate_batch_sequential(self, prompts: List[str], max_new_tokens: int = 128) -> List[str]:\n",
    "        # llama.cpp-python doesn't do multi-prompt chat in one call cleanly,\n",
    "        # so we just run them sequentially on the same loaded llm.\n",
    "        outputs = []\n",
    "        for p in prompts:\n",
    "            outputs.append(self._generate_one(p, max_new_tokens=max_new_tokens))\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cac1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "global orchestrator, experts, num_loaded\n",
    "orchestrator = OrchestratorRouter(ORCHESTRATOR_MODEL_PATH)\n",
    "\n",
    "experts = {\n",
    "    label: GgufExpert(\n",
    "        name=label,\n",
    "        model_path=gguf_path,\n",
    "    )\n",
    "    for label, gguf_path in GGUF_MODELS.items()\n",
    "}\n",
    "\n",
    "num_loaded = 0\n",
    "\n",
    "if ENABLE_BATCHING:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    for exp in experts.values():\n",
    "        exp.request_queue = asyncio.Queue()\n",
    "        exp.worker_task = loop.create_task(batch_worker(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4cc0417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routed to expert 'anxiety'\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "preprocess_start = t0\n",
    "\n",
    "# orchestrator routing\n",
    "orch_start = time.perf_counter()\n",
    "prompt = \"I have no freinds. will you be my friend?\"\n",
    "label = orchestrator.route(prompt)\n",
    "orch_end = time.perf_counter()\n",
    "preprocess_end = orch_end\n",
    "chosen_expert = experts[label]\n",
    "print(f\"Routed to expert '{label}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "829a1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_time_ms = 0.0\n",
    "batch_size = 1\n",
    "\n",
    "chosen_expert.inc_active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51d6f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from ./models/gguf/anxiety-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Anxiety_Fp16\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 1.2B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MEM] Loading GGUF for expert 'anxiety' from ./models/gguf/anxiety-q8_0.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_sep_token bool             = false\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type q8_0:  113 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 1.22 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 16\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.24 B\n",
      "print_info: general.name     = Anxiety_Fp16\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 162 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  1252.41 MiB\n",
      "..............................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 128\n",
      "llama_context: n_ubatch      = 128\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   128.00 MiB\n",
      "llama_kv_cache_unified: size =  128.00 MiB (  4096 cells,  16 layers,  1/1 seqs), K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 1176\n",
      "llama_context: worst-case: n_tokens = 128, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  128, n_seqs =  1, n_outputs =  128\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  128, n_seqs =  1, n_outputs =  128\n",
      "llama_context:        CPU compute buffer size =    70.50 MiB\n",
      "llama_context: graph nodes  = 566\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Anxiety_Fp16', 'general.architecture': 'llama', 'general.type': 'model', 'llama.feed_forward_length': '8192', 'general.size_label': '1.2B', 'llama.block_count': '16', 'llama.context_length': '131072', 'llama.embedding_length': '2048', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '64', 'llama.attention.value_length': '64', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.bos_token_id': '128000', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_sep_token': 'false', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "load_time_ms = await asyncio.to_thread(chosen_expert.ensure_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aeca9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1165.29 ms\n",
      "llama_perf_context_print: prompt eval time =    1164.92 ms /    47 tokens (   24.79 ms per token,    40.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5943.24 ms /   127 runs   (   46.80 ms per token,    21.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    7342.69 ms /   174 tokens\n",
      "llama_perf_context_print:    graphs reused =        122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inf_start = time.perf_counter()\n",
    "output = await asyncio.to_thread(chosen_expert.generate_single, prompt)\n",
    "inf_end = time.perf_counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e4980de",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_expert.dec_active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e44feae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elapsed_ms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inf_time_ms \u001b[38;5;241m=\u001b[39m \u001b[43melapsed_ms\u001b[49m(inf_start, inf_end)\n\u001b[0;32m      3\u001b[0m t_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'elapsed_ms' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "inf_time_ms = elapsed_ms(inf_start, inf_end)\n",
    "\n",
    "t_end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b37c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
