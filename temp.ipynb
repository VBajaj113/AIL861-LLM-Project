{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35acac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies (run this once)\n",
    "!pip install -q \"transformers>=4.44.0\" \"accelerate>=0.33.0\" \"datasets>=2.20.0\" \\\n",
    "              \"peft>=0.11.0\" bitsandbytes sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6967f317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA visible devices: 3\n",
      "PyTorch sees devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and global config\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # maps cuda:3 to local device 0 for this process\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "print(\"CUDA visible devices:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"PyTorch sees devices:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6a98779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anxiety': PosixPath('data/anxiety_data.json'), 'depression': PosixPath('data/depression_data.json'), 'bipolar': PosixPath('data/bipolar_data.json'), 'ocd': PosixPath('data/ocd_data.json'), 'schizophrenia': PosixPath('data/schizophrenia_data.json')}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Path config (EDIT these paths/names if needed)\n",
    "\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "RAW_DATA_DIR = Path(\"data\")\n",
    "PROCESSED_DIR = Path(\"processed_sft\")\n",
    "OUTPUT_DIR = Path(\"experts_checkpoints\")\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Map disorder -> raw json path\n",
    "# TODO: update all 5 paths to your actual files.\n",
    "DISORDER_CONFIGS: Dict[str, Path] = {\n",
    "    \"anxiety\": RAW_DATA_DIR / \"anxiety_data.json\",\n",
    "    \"depression\": RAW_DATA_DIR / \"depression_data.json\",\n",
    "    \"bipolar\": RAW_DATA_DIR / \"bipolar_data.json\",\n",
    "    \"ocd\": RAW_DATA_DIR / \"ocd_data.json\",\n",
    "    \"schizophrenia\": RAW_DATA_DIR / \"schizophrenia_data.json\",\n",
    "}\n",
    "\n",
    "print(DISORDER_CONFIGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6118bcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded. EOS token: <|eot_id|> | PAD token: <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load tokenizer (shared across everything)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "# Make sure we have a pad token for batching (LLama usually uses eos as pad)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded. EOS token:\", tokenizer.eos_token, \"| PAD token:\", tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b11a4b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing anxiety from data/anxiety_data.json ...\n",
      "  -> kept 740 examples\n",
      "  -> saved to processed_sft/anxiety_sft.jsonl\n",
      "\n",
      "Processing depression from data/depression_data.json ...\n",
      "  -> kept 750 examples\n",
      "  -> saved to processed_sft/depression_sft.jsonl\n",
      "\n",
      "Processing disorder3 from data/bipolar_data.json ...\n",
      "  -> kept 750 examples\n",
      "  -> saved to processed_sft/disorder3_sft.jsonl\n",
      "\n",
      "Processing disorder4 from data/ocd_data.json ...\n",
      "  -> kept 740 examples\n",
      "  -> saved to processed_sft/disorder4_sft.jsonl\n",
      "\n",
      "Processing disorder5 from data/schizophrenia_data.json ...\n",
      "  -> kept 745 examples\n",
      "  -> saved to processed_sft/disorder5_sft.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Preprocess raw JSON into SFT-ready prompts and save as JSONL\n",
    "\n",
    "def load_raw_pairs(json_path: Path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    # Handle both: list[...] or {\"data\": [...]}\n",
    "    if isinstance(obj, dict) and \"data\" in obj:\n",
    "        data = obj[\"data\"]\n",
    "    else:\n",
    "        data = obj\n",
    "    assert isinstance(data, list), f\"Expected list of examples in {json_path}\"\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_chat_prompt(instruction: str, output: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a proper chat-style prompt for Llama-3.2-Instruct using the tokenizer's template.\n",
    "    We keep full sequence (user + assistant) for causal LM training.\n",
    "    \"\"\"\n",
    "    instruction = instruction.strip()\n",
    "    output = output.strip()\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"assistant\", \"content\": output},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def preprocess_and_save(disorder_name: str, json_path: Path, min_char_len: int = 20):\n",
    "    print(f\"Processing {disorder_name} from {json_path} ...\")\n",
    "    raw_data = load_raw_pairs(json_path)\n",
    "\n",
    "    processed = []\n",
    "    for ex in raw_data:\n",
    "        instr = ex.get(\"instruction\", \"\")\n",
    "        out = ex.get(\"output\", \"\")\n",
    "        if not instr or not out:\n",
    "            continue\n",
    "        if len(instr) < min_char_len and len(out) < min_char_len:\n",
    "            continue\n",
    "\n",
    "        text = build_chat_prompt(instr, out)\n",
    "        processed.append({\"text\": text})\n",
    "\n",
    "    print(f\"  -> kept {len(processed)} examples\")\n",
    "\n",
    "    out_path = PROCESSED_DIR / f\"{disorder_name}_sft.jsonl\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in processed:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"  -> saved to {out_path}\\n\")\n",
    "\n",
    "\n",
    "for disorder, path in DISORDER_CONFIGS.items():\n",
    "    preprocess_and_save(disorder, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f5c6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Helpers for loading tokenized dataset for a given disorder\n",
    "\n",
    "MAX_SEQ_LEN = 512  # you can push to 768 if you really want, but 512 is fine\n",
    "\n",
    "def load_sft_dataset_for_disorder(disorder_name: str):\n",
    "    jsonl_path = PROCESSED_DIR / f\"{disorder_name}_sft.jsonl\"\n",
    "    assert jsonl_path.exists(), f\"Preprocessed file not found: {jsonl_path}\"\n",
    "\n",
    "    dataset = load_dataset(\"json\", data_files={\"train\": str(jsonl_path)})\n",
    "    dataset = dataset[\"train\"]  # single split\n",
    "\n",
    "    # Train/validation split (e.g., 90/10)\n",
    "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_ds = dataset[\"train\"]\n",
    "    val_ds = dataset[\"test\"]\n",
    "\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=False,  # dynamic padding via data collator\n",
    "        )\n",
    "\n",
    "    train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "    val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    print(\n",
    "        f\"{disorder_name}: train={len(train_ds)}, val={len(val_ds)}, \"\n",
    "        f\"sample input_ids length={len(train_ds[0]['input_ids'])}\"\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "385d8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: LoRA config and model loader\n",
    "\n",
    "def create_lora_model():\n",
    "    # Load base model on the single visible GPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,   # A100 is chill with bf16\n",
    "        device_map={\"\": 0},           # single GPU (after CUDA_VISIBLE_DEVICES remap)\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # LLaMA-style attention + MLP modules\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed696f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA-style model loader: 4-bit quantized base + LoRA on top\n",
    "\n",
    "def create_lora_model_qlora():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,              # main switch\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",      # NF4 is standard for QLoRA\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,  # A100 = bf16 friendly\n",
    "    )\n",
    "\n",
    "    print(\"Loading 4-bit quantized base model...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},   # because we set CUDA_VISIBLE_DEVICES=3 earlier\n",
    "        trust_remote_code=False,\n",
    "    )\n",
    "\n",
    "    # LoRA config – same as before\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()  # sanity check: only a few M params trainable\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b49ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Trainer setup for a single disorder\n",
    "\n",
    "def train_expert(\n",
    "    disorder_name: str,\n",
    "    num_epochs: int = 8,\n",
    "    per_device_train_batch_size: int = 8,\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "    learning_rate: float = 2e-4,\n",
    "    warmup_ratio: float = 0.03,\n",
    "):\n",
    "    print(f\"===== Training expert for disorder: {disorder_name} =====\")\n",
    "\n",
    "    train_ds, val_ds = load_sft_dataset_for_disorder(disorder_name)\n",
    "    model = create_lora_model()\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    output_dir = OUTPUT_DIR / disorder_name\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,   # still pass val_ds so evaluate() can use it\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # ---- TRAIN ----\n",
    "    train_result = trainer.train()\n",
    "    print(\"Train result:\", train_result.metrics)\n",
    "\n",
    "    # ---- EVAL ON VAL SET ----\n",
    "    eval_metrics = trainer.evaluate()   # uses eval_dataset=val_ds from above\n",
    "    print(f\"Validation metrics for {disorder_name}:\")\n",
    "    for k, v in eval_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\" if isinstance(v, (float, int)) else f\"  {k}: {v}\")\n",
    "\n",
    "    # Save LoRA adapter + tokenizer\n",
    "    print(f\"Saving LoRA adapter and tokenizer for {disorder_name} to {output_dir}\")\n",
    "    model.save_pretrained(str(output_dir))\n",
    "    tokenizer.save_pretrained(str(output_dir))\n",
    "\n",
    "    # OPTIONAL: save metrics to a file for your report\n",
    "    metrics_path = output_dir / \"metrics.json\"\n",
    "    import json\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"train_metrics\": train_result.metrics,\n",
    "                \"eval_metrics\": eval_metrics,\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "    print(f\"Saved metrics to {metrics_path}\")\n",
    "\n",
    "    # Free GPU memory (important if training multiple experts sequentially)\n",
    "    del trainer\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"===== Done: {disorder_name} =====\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b076ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_expert_qlora(\n",
    "    disorder_name: str,\n",
    "    num_epochs: int = 3,\n",
    "    per_device_train_batch_size: int = 8,\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "    learning_rate: float = 5e-5,\n",
    "    warmup_ratio: float = 0.03,\n",
    "):\n",
    "    print(f\"===== Training expert for disorder: {disorder_name} (QLoRA 4-bit) =====\")\n",
    "\n",
    "    train_ds, val_ds = load_sft_dataset_for_disorder(disorder_name)\n",
    "    model = create_lora_model_qlora()   # <-- changed here\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    output_dir = OUTPUT_DIR / disorder_name\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "    print(\"Train metrics:\", train_result.metrics)\n",
    "\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(\"Eval metrics:\", eval_metrics)\n",
    "\n",
    "    print(f\"Saving QLoRA expert for {disorder_name} to {output_dir}\")\n",
    "    model.save_pretrained(str(output_dir))\n",
    "    tokenizer.save_pretrained(str(output_dir))\n",
    "\n",
    "    del trainer, model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"===== Done: {disorder_name} =====\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14dc5192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training expert for disorder: anxiety =====\n",
      "anxiety: train=666, val=74, sample input_ids length=172\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_2559243/944010587.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 00:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.356600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.117400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train result: {'train_runtime': 38.2624, 'train_samples_per_second': 52.218, 'train_steps_per_second': 6.586, 'total_flos': 2268383284887552.0, 'train_loss': 1.4412491463479542, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics for anxiety:\n",
      "  eval_loss: 1.6061\n",
      "  eval_runtime: 0.6199\n",
      "  eval_samples_per_second: 119.3780\n",
      "  eval_steps_per_second: 16.1320\n",
      "  epoch: 3.0000\n",
      "Saving LoRA adapter and tokenizer for anxiety to experts_checkpoints/anxiety\n",
      "Saved metrics to experts_checkpoints/anxiety/metrics.json\n",
      "===== Done: anxiety =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 9A: Train a single expert (e.g., anxiety) first\n",
    "\n",
    "TARGET_DISORDER = \"anxiety\"  # change this to any key in DISORDER_CONFIGS\n",
    "\n",
    "train_expert(\n",
    "    disorder_name=TARGET_DISORDER,\n",
    "    num_epochs=3,                     # reduce if you need faster runs\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ac2fd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expert from: experts_checkpoints/anxiety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 Nov 2025\n",
      "\n",
      "user\n",
      "\n",
      "What are common physical sensations someone with anxiety might experience?assistant\n",
      "\n",
      "Individuals experiencing anxiety may report experiencing a variety of physical sensations such as tingling or shivering, coldness or heat, trembling or shaking, muscle tension, joint stiffness, restlessness, or feeling 'driven mad' by bodily sensations like pins and needles, numbness, or a feeling of impending doom when in certain physical positions. They might also have difficulty relaxing and may experience muscle inflexibility. Some individuals report specific phobic-related sensations such as heart pounding, a feeling of choking or suffocation, or being 'trapped' in a situation. These are not diagnostic criteria but common subjective experiences associated with anxiety. The intensity and frequency of these sensations can vary widely among individuals. Some individuals may experience fewer of these sensations and may have more difficulty articulating their physical experiences. For instance, compared to controls, individuals with specific phobias may report fewer subjective physical sensations and have less difficulty describing them. However, they may also experience a greater number of them overall. The key is recognizing these physical sensations as part of the anxiety experience. Research is needed into their nature, causes, and consequences for individuals with anxiety. Experiments involving voluntary hyperventilation have also yielded some key findings. For example, a reduction in CO2 levels has been observed during hyp\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Sanity check generation with one expert\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "TEST_DISORDER = \"anxiety\"  # pick one that you already trained\n",
    "expert_dir = OUTPUT_DIR / TEST_DISORDER\n",
    "\n",
    "print(\"Loading expert from:\", expert_dir)\n",
    "\n",
    "expert_model = AutoModelForCausalLM.from_pretrained(\n",
    "    expert_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "expert_tokenizer = AutoTokenizer.from_pretrained(expert_dir)\n",
    "\n",
    "def generate_answer(prompt: str, max_new_tokens: int = 256):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    encoded = expert_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(expert_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = expert_model.generate(\n",
    "            encoded,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    decoded = expert_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "test_query = \"What are common physical sensations someone with anxiety might experience?\"\n",
    "print(generate_answer(test_query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114545b1",
   "metadata": {},
   "source": [
    "3 epoch:Individuals with anxiety disorders often report a variety of physical sensations, including sensations of nervousness or fatigue, muscle tension, a 'pins and needles' sensation in extremities, and even experiencing heat or cold. Some may also report sensations like a 'fullness' or 'gulping' in their chest or stomach. A few individuals might experience more unusual sensations, such as an unexplained 'heavy feeling' in their chest or a 'burning' sensation in their hands. While these sensations are generally not indicative of an underlying medical condition, they can be distressing enough to warrant attention. Clinically, it's crucial to distinguish these physical sensations from the physical symptoms of medical conditions, especially cardiovascular diseases, which can be exacerbated by anxiety. A thorough medical evaluation is necessary to rule out any underlying health issues. The focus of treatment should then be on the anxiety itself, rather than solely on managing the physical symptoms. Techniques like cognitive-behavioral therapy (CBT) and relaxation exercises can be very beneficial in addressing these sensations and the underlying anxiety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0e09d",
   "metadata": {},
   "source": [
    "1 epoch: Common physical sensations experienced by individuals with anxiety include: a sense of heaviness or fullness in the body, a feeling of numbness or coldness, a general feeling of being \"drained\" or exhausted, and a sensation of being \"overwhelmed\" or suffocated. These sensations can be accompanied by a range of other physical symptoms, including rapid heartbeat, sweating, and trembling. For some, these sensations might be intensely debilitating, while for others, they might be more tolerable. They can also be linked to specific situations or contexts, such as a public speaking event or a stressful work meeting. These experiences can be highly distressing and interfere with daily life. They might also be directly associated with panic symptoms, such as shortness of breath, lightheadedness, and a feeling of impending doom. The physical sensations can serve as a warning sign, prompting the individual to seek a safe space or to prepare for a potentially intense event. They can also be a source of chronic pain or discomfort, especially if the individual has experienced traumatic events or has a history of chronic anxiety. The intensity and duration of these sensations can vary widely, and they can be exacerbated by stress, certain medications, and other factors. They are not just a normal response to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f5b04",
   "metadata": {},
   "source": [
    "5 epoch : Individuals experiencing anxiety may report experiencing a variety of physical sensations such as tingling or shivering, coldness or heat, trembling or shaking, muscle tension, joint stiffness, restlessness, or feeling 'driven mad' by bodily sensations like pins and needles, numbness, or a feeling of impending doom when in certain physical positions. They might also have difficulty relaxing and may experience muscle inflexibility. Some individuals report specific phobic-related sensations such as heart pounding, a feeling of choking or suffocation, or being 'trapped' in a situation. These are not diagnostic criteria but common subjective experiences associated with anxiety. The intensity and frequency of these sensations can vary widely among individuals. Some individuals may experience fewer of these sensations and may have more difficulty articulating their physical experiences. For instance, compared to controls, individuals with specific phobias may report fewer subjective physical sensations and have less difficulty describing them. However, they may also experience a greater number of them overall. The key is recognizing these physical sensations as part of the anxiety experience. Research is needed into their nature, causes, and consequences for individuals with anxiety. Experiments involving voluntary hyperventilation have also yielded some key findings. For example, a reduction in CO2 levels has been observed during hyp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0854afc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training expert for disorder: anxiety (QLoRA 4-bit) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 740 examples [00:00, 76125.40 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 5220.96 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 5354.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anxiety: train=666, val=74, sample input_ids length=172\n",
      "Loading 4-bit quantized base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_3612019/3762583956.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 00:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.801400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.712000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.532100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6924580d-357959af3fc509273b47f85a;5d8587b2-d69e-4ce6-8c0d-b50582052507)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: {'train_runtime': 49.0731, 'train_samples_per_second': 40.715, 'train_steps_per_second': 5.135, 'total_flos': 2268383284887552.0, 'train_loss': 1.7856839573572552, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 1.642885446548462, 'eval_runtime': 0.7994, 'eval_samples_per_second': 92.565, 'eval_steps_per_second': 12.509, 'epoch': 3.0}\n",
      "Saving QLoRA expert for anxiety to experts_checkpoints/anxiety\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6924580e-270f16762dd9d6451e805fba;17a31d0b-90ea-40a8-b236-12b2fb9faf70)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Done: anxiety =====\n",
      "\n",
      "===== Training expert for disorder: depression (QLoRA 4-bit) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 750 examples [00:00, 78255.83 examples/s]\n",
      "Map: 100%|██████████| 675/675 [00:00<00:00, 7180.52 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6369.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depression: train=675, val=75, sample input_ids length=141\n",
      "Loading 4-bit quantized base model...\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_3612019/3762583956.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.449600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-69245848-6400d4a85ee67a2f0ff64504;59a6cd8b-78de-464b-b090-70439e7dd884)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: {'train_runtime': 51.8576, 'train_samples_per_second': 39.049, 'train_steps_per_second': 4.917, 'total_flos': 2191758132940800.0, 'train_loss': 1.7005548140581916, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 1.5458179712295532, 'eval_runtime': 0.7349, 'eval_samples_per_second': 102.052, 'eval_steps_per_second': 13.607, 'epoch': 3.0}\n",
      "Saving QLoRA expert for depression to experts_checkpoints/depression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6924584a-7aab8e4835debe47271e72ca;b86a689f-a2a4-4917-bcc5-5412a55358d3)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Done: depression =====\n",
      "\n",
      "===== Training expert for disorder: disorder3 (QLoRA 4-bit) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 750 examples [00:00, 115329.52 examples/s]\n",
      "Map: 100%|██████████| 675/675 [00:00<00:00, 8525.16 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 5265.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disorder3: train=675, val=75, sample input_ids length=82\n",
      "Loading 4-bit quantized base model...\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_3612019/3762583956.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.594500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.499100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-69245882-35c15df25c5d824b58173f16;7628e1ec-c050-430d-a96c-0be8cd6f7364)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: {'train_runtime': 51.5039, 'train_samples_per_second': 39.317, 'train_steps_per_second': 4.951, 'total_flos': 2255530719510528.0, 'train_loss': 1.7417336819218654, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 1.5625090599060059, 'eval_runtime': 0.758, 'eval_samples_per_second': 98.948, 'eval_steps_per_second': 13.193, 'epoch': 3.0}\n",
      "Saving QLoRA expert for disorder3 to experts_checkpoints/disorder3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-69245883-09cb90e154c1905718625884;0de1b147-5c92-4bf1-9c57-e89ca51c4c89)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Done: disorder3 =====\n",
      "\n",
      "===== Training expert for disorder: disorder4 (QLoRA 4-bit) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 740 examples [00:00, 116124.85 examples/s]\n",
      "Map: 100%|██████████| 666/666 [00:00<00:00, 8003.16 examples/s]\n",
      "Map: 100%|██████████| 74/74 [00:00<00:00, 6688.04 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disorder4: train=666, val=74, sample input_ids length=139\n",
      "Loading 4-bit quantized base model...\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_3612019/3762583956.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 00:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.835700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.781400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.706400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.589200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-692458bc-6cc8660c51db5ec64d9f191d;a1b278dd-d41f-4533-afdf-2b2b5d478736)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: {'train_runtime': 50.902, 'train_samples_per_second': 39.252, 'train_steps_per_second': 4.951, 'total_flos': 2281235850264576.0, 'train_loss': 1.8516639130456107, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 1.6663367748260498, 'eval_runtime': 0.7562, 'eval_samples_per_second': 97.852, 'eval_steps_per_second': 13.223, 'epoch': 3.0}\n",
      "Saving QLoRA expert for disorder4 to experts_checkpoints/disorder4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-692458bd-64bd04f51e517ad50327f46f;7bcc15ff-8a4d-4492-b808-e3f5e158814c)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Done: disorder4 =====\n",
      "\n",
      "===== Training expert for disorder: disorder5 (QLoRA 4-bit) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 745 examples [00:00, 120908.39 examples/s]\n",
      "Map: 100%|██████████| 670/670 [00:00<00:00, 8820.36 examples/s]\n",
      "Map: 100%|██████████| 75/75 [00:00<00:00, 6805.69 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disorder5: train=670, val=75, sample input_ids length=117\n",
      "Loading 4-bit quantized base model...\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_3612019/3762583956.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 00:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.223700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.706200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.619400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.452300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-692458f6-00bf69b52619e4b606fcaf3a;f392081a-1a32-4898-a82f-176b6eb8be8c)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics: {'train_runtime': 51.2122, 'train_samples_per_second': 39.248, 'train_steps_per_second': 4.921, 'total_flos': 2118960399360000.0, 'train_loss': 1.7052358606505016, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 1.5469661951065063, 'eval_runtime': 0.7274, 'eval_samples_per_second': 103.113, 'eval_steps_per_second': 13.748, 'epoch': 3.0}\n",
      "Saving QLoRA expert for disorder5 to experts_checkpoints/disorder5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-692458f8-3ca78b766efbec313fa0db9c;0c0ca403-93aa-4321-9078-731df0c3b789)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/faiz/scratch/anaconda3/envs/faiz/lib/python3.12/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Done: disorder5 =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for disorder_name in DISORDER_CONFIGS.keys():\n",
    "    train_expert_qlora(\n",
    "        disorder_name=disorder_name,\n",
    "        num_epochs=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22662883",
   "metadata": {},
   "source": [
    "## GGUF Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93c06c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Loading LoRA adapter...\n",
      "Merging LoRA weights into base model...\n",
      "Saving merged FP16 model to: merged_experts/schizophrenia_fp16\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "EXPERT_DIR = Path(\"experts_checkpoints/schizophrenia\")      # LoRA checkpoint we trained\n",
    "MERGED_OUT_DIR = Path(\"merged_experts/schizophrenia_fp16\")  # where we'll save merged full model\n",
    "\n",
    "MERGED_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Load base model in full precision (CPU or single GPU)\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,   # or bfloat16, but fp16 is common for GGUF conversion\n",
    "    device_map=\"cpu\",            # can use {\"\": 0} if you want GPU\n",
    ")\n",
    "\n",
    "# 2) Load LoRA weights on top of base\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    EXPERT_DIR,\n",
    ")\n",
    "\n",
    "# 3) Merge LoRA into base weights\n",
    "print(\"Merging LoRA weights into base model...\")\n",
    "model = model.merge_and_unload()   # returns a plain AutoModelForCausalLM with merged weights\n",
    "\n",
    "# 4) Save merged model + tokenizer in HF format\n",
    "print(\"Saving merged FP16 model to:\", MERGED_OUT_DIR)\n",
    "model.save_pretrained(MERGED_OUT_DIR)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "tokenizer.save_pretrained(MERGED_OUT_DIR)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ef508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca9aafe8",
   "metadata": {},
   "source": [
    "# Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab44288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "  0: anxiety\n",
      "  1: bipolar\n",
      "  2: depression\n",
      "  3: ocd\n",
      "  4: schizophrenia\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and path / label config\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# Root dirs\n",
    "DATA_DIR = Path(\"data\")\n",
    "ORCH_DIR = Path(\"orchestrator\")\n",
    "ORCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Map disorder name -> json path\n",
    "# TODO: adjust to match your actual filenames\n",
    "DISORDER_FILES = {\n",
    "    \"anxiety\": RAW_DATA_DIR / \"anxiety_data.json\",\n",
    "    \"depression\": RAW_DATA_DIR / \"depression_data.json\",\n",
    "    \"bipolar\": RAW_DATA_DIR / \"bipolar_data.json\",\n",
    "    \"ocd\": RAW_DATA_DIR / \"ocd_data.json\",\n",
    "    \"schizophrenia\": RAW_DATA_DIR / \"schizophrenia_data.json\",\n",
    "}\n",
    "\n",
    "# Assign integer labels in a fixed order\n",
    "LABEL2ID = {name: i for i, name in enumerate(sorted(DISORDER_FILES.keys()))}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for name, idx in LABEL2ID.items():\n",
    "    print(f\"  {idx}: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83deab7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading anxiety from data/anxiety_data.json\n",
      "Loading depression from data/depression_data.json\n",
      "Loading bipolar from data/bipolar_data.json\n",
      "Loading ocd from data/ocd_data.json\n",
      "Loading schizophrenia from data/schizophrenia_data.json\n",
      "\n",
      "Dataset summary:\n",
      "label_name\n",
      "depression       750\n",
      "bipolar          750\n",
      "schizophrenia    745\n",
      "anxiety          740\n",
      "ocd              740\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total examples: 3725\n",
      "\n",
      "Saved combined dataset to orchestrator/orchestrator_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load JSONs and build a single DataFrame: text, label_id, label_name\n",
    "\n",
    "def load_json_list(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    # Handle both:\n",
    "    # - [ {instruction, output, ...}, ... ]\n",
    "    # - { \"data\": [ ... ] }\n",
    "    if isinstance(obj, dict) and \"data\" in obj:\n",
    "        data = obj[\"data\"]\n",
    "    else:\n",
    "        data = obj\n",
    "    assert isinstance(data, list), f\"Expected list in {path}, got {type(data)}\"\n",
    "    return data\n",
    "\n",
    "rows = []\n",
    "\n",
    "for disorder_name, json_path in DISORDER_FILES.items():\n",
    "    print(f\"Loading {disorder_name} from {json_path}\")\n",
    "    examples = load_json_list(json_path)\n",
    "    label_id = LABEL2ID[disorder_name]\n",
    "\n",
    "    for ex in examples:\n",
    "        instr = ex.get(\"instruction\", \"\")\n",
    "        if not instr or not isinstance(instr, str):\n",
    "            continue\n",
    "        instr = instr.strip()\n",
    "        if len(instr) < 10:  # filter extremely short junk\n",
    "            continue\n",
    "        rows.append(\n",
    "            {\n",
    "                \"text\": instr,\n",
    "                \"label_id\": label_id,\n",
    "                \"label_name\": disorder_name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"\\nDataset summary:\")\n",
    "print(df[\"label_name\"].value_counts())\n",
    "print(\"\\nTotal examples:\", len(df))\n",
    "\n",
    "# Optional: save for inspection / reproducibility\n",
    "csv_path = ORCH_DIR / \"orchestrator_dataset.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSaved combined dataset to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd937b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "  train: 2607\n",
      "  val  : 559\n",
      "  test : 559\n",
      "\n",
      "Train label distribution:\n",
      "label_name\n",
      "bipolar          525\n",
      "depression       525\n",
      "schizophrenia    521\n",
      "ocd              518\n",
      "anxiety          518\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val label distribution:\n",
      "label_name\n",
      "bipolar          113\n",
      "depression       112\n",
      "schizophrenia    112\n",
      "ocd              111\n",
      "anxiety          111\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "label_name\n",
      "depression       113\n",
      "schizophrenia    112\n",
      "bipolar          112\n",
      "ocd              111\n",
      "anxiety          111\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Stratified train/val/test split\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# First: train+val vs test\n",
    "df_train_val, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.15,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df[\"label_id\"],\n",
    ")\n",
    "\n",
    "# Then: split train vs val\n",
    "df_train, df_val = train_test_split(\n",
    "    df_train_val,\n",
    "    test_size=0.1765,  # ~15% of total → so final split ≈ 70/15/15\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df_train_val[\"label_id\"],\n",
    ")\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(\"  train:\", len(df_train))\n",
    "print(\"  val  :\", len(df_val))\n",
    "print(\"  test :\", len(df_test))\n",
    "\n",
    "def describe_split(df_split, name):\n",
    "    print(f\"\\n{name} label distribution:\")\n",
    "    print(df_split[\"label_name\"].value_counts())\n",
    "\n",
    "describe_split(df_train, \"Train\")\n",
    "describe_split(df_val, \"Val\")\n",
    "describe_split(df_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "564a0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define models and helper to train & evaluate\n",
    "\n",
    "def make_tfidf():\n",
    "    return TfidfVectorizer(\n",
    "        ngram_range=(1, 2),      # unigrams + bigrams\n",
    "        max_features=20_000,\n",
    "        min_df=2,\n",
    "        stop_words=\"english\",\n",
    "    )\n",
    "\n",
    "def train_and_eval_model(model_name: str, clf):\n",
    "    print(f\"\\n===== Training {model_name} =====\")\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"tfidf\", make_tfidf()),\n",
    "            (\"clf\", clf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    X_train, y_train = df_train[\"text\"].tolist(), df_train[\"label_id\"].values\n",
    "    X_val, y_val = df_val[\"text\"].tolist(), df_val[\"label_id\"].values\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on val\n",
    "    y_pred = pipe.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    print(f\"{model_name} val accuracy: {acc:.4f}\")\n",
    "    print(f\"{model_name} val macro-precision: {precision:.4f}\")\n",
    "    print(f\"{model_name} val macro-recall:    {recall:.4f}\")\n",
    "    print(f\"{model_name} val macro-F1:        {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_val,\n",
    "            y_pred,\n",
    "            target_names=[ID2LABEL[i] for i in sorted(ID2LABEL.keys())],\n",
    "            zero_division=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[f\"true_{ID2LABEL[i]}\" for i in sorted(ID2LABEL.keys())],\n",
    "        columns=[f\"pred_{ID2LABEL[i]}\" for i in sorted(ID2LABEL.keys())],\n",
    "    )\n",
    "    print(\"Confusion matrix:\")\n",
    "    display(cm_df)\n",
    "\n",
    "    metrics = {\n",
    "        \"val_accuracy\": acc,\n",
    "        \"val_macro_precision\": precision,\n",
    "        \"val_macro_recall\": recall,\n",
    "        \"val_macro_f1\": f1,\n",
    "    }\n",
    "\n",
    "    return pipe, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab8e9cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training LogisticRegression =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression val accuracy: 0.8962\n",
      "LogisticRegression val macro-precision: 0.8975\n",
      "LogisticRegression val macro-recall:    0.8965\n",
      "LogisticRegression val macro-F1:        0.8965\n",
      "\n",
      "Per-class metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      anxiety       0.92      0.94      0.93       111\n",
      "      bipolar       0.89      0.81      0.85       113\n",
      "   depression       0.79      0.84      0.81       112\n",
      "          ocd       0.91      0.92      0.91       111\n",
      "schizophrenia       0.97      0.97      0.97       112\n",
      "\n",
      "     accuracy                           0.90       559\n",
      "    macro avg       0.90      0.90      0.90       559\n",
      " weighted avg       0.90      0.90      0.90       559\n",
      "\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_anxiety</th>\n",
       "      <th>pred_bipolar</th>\n",
       "      <th>pred_depression</th>\n",
       "      <th>pred_ocd</th>\n",
       "      <th>pred_schizophrenia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_anxiety</th>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_bipolar</th>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_depression</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>94</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_ocd</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_schizophrenia</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pred_anxiety  pred_bipolar  pred_depression  pred_ocd  \\\n",
       "true_anxiety                 104             0                3         4   \n",
       "true_bipolar                   1            92               16         2   \n",
       "true_depression                2            11               94         4   \n",
       "true_ocd                       5             0                4       102   \n",
       "true_schizophrenia             1             0                2         0   \n",
       "\n",
       "                    pred_schizophrenia  \n",
       "true_anxiety                         0  \n",
       "true_bipolar                         2  \n",
       "true_depression                      1  \n",
       "true_ocd                             0  \n",
       "true_schizophrenia                 109  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training MultinomialNB =====\n",
      "MultinomialNB val accuracy: 0.8640\n",
      "MultinomialNB val macro-precision: 0.8642\n",
      "MultinomialNB val macro-recall:    0.8643\n",
      "MultinomialNB val macro-F1:        0.8614\n",
      "\n",
      "Per-class metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      anxiety       0.92      0.92      0.92       111\n",
      "      bipolar       0.76      0.86      0.81       113\n",
      "   depression       0.82      0.64      0.72       112\n",
      "          ocd       0.89      0.95      0.92       111\n",
      "schizophrenia       0.93      0.96      0.94       112\n",
      "\n",
      "     accuracy                           0.86       559\n",
      "    macro avg       0.86      0.86      0.86       559\n",
      " weighted avg       0.86      0.86      0.86       559\n",
      "\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_anxiety</th>\n",
       "      <th>pred_bipolar</th>\n",
       "      <th>pred_depression</th>\n",
       "      <th>pred_ocd</th>\n",
       "      <th>pred_schizophrenia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_anxiety</th>\n",
       "      <td>102</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_bipolar</th>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_depression</th>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_ocd</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_schizophrenia</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pred_anxiety  pred_bipolar  pred_depression  pred_ocd  \\\n",
       "true_anxiety                 102             2                2         5   \n",
       "true_bipolar                   0            97               11         3   \n",
       "true_depression                4            26               72         4   \n",
       "true_ocd                       3             0                3       105   \n",
       "true_schizophrenia             2             2                0         1   \n",
       "\n",
       "                    pred_schizophrenia  \n",
       "true_anxiety                         0  \n",
       "true_bipolar                         2  \n",
       "true_depression                      6  \n",
       "true_ocd                             0  \n",
       "true_schizophrenia                 107  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Summary (val macro-F1) =====\n",
      "LogReg: 0.8965\n",
      "NB    : 0.8614\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Train both models and compare on validation set\n",
    "\n",
    "logreg_clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "\n",
    "logreg_pipe, logreg_metrics = train_and_eval_model(\"LogisticRegression\", logreg_clf)\n",
    "nb_pipe, nb_metrics = train_and_eval_model(\"MultinomialNB\", nb_clf)\n",
    "\n",
    "print(\"\\n===== Summary (val macro-F1) =====\")\n",
    "print(f\"LogReg: {logreg_metrics['val_macro_f1']:.4f}\")\n",
    "print(f\"NB    : {nb_metrics['val_macro_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e08b1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final Logistic Regression orchestrator on FULL dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "Saved final orchestrator pipeline to: orchestrator/orchestrator_model_full_logreg.joblib\n",
      "Saved label mapping to: orchestrator/label_mapping.json\n"
     ]
    }
   ],
   "source": [
    "# Cell: Train final orchestrator (Logistic Regression) on the full dataset and save\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "print(\"Training final Logistic Regression orchestrator on FULL dataset...\")\n",
    "\n",
    "# Features and labels from the full combined dataset\n",
    "X_all = df[\"text\"].tolist()\n",
    "y_all = df[\"label_id\"].values\n",
    "\n",
    "# Define final TF-IDF + LogReg pipeline\n",
    "final_logreg_clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "final_orchestrator = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", make_tfidf()),   # reuse the same TF-IDF config function\n",
    "        (\"clf\", final_logreg_clf),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on all data\n",
    "final_orchestrator.fit(X_all, y_all)\n",
    "\n",
    "# Save the pipeline\n",
    "final_model_path = ORCH_DIR / \"orchestrator_model_full_logreg.joblib\"\n",
    "joblib.dump(final_orchestrator, final_model_path)\n",
    "\n",
    "# Save label mapping (so inference code can map ids <-> names)\n",
    "labelmap_path = ORCH_DIR / \"label_mapping.json\"\n",
    "with open(labelmap_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"LABEL2ID\": LABEL2ID,\n",
    "            \"ID2LABEL\": ID2LABEL,\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(f\"Saved final orchestrator pipeline to: {final_model_path}\")\n",
    "print(f\"Saved label mapping to: {labelmap_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a228376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
